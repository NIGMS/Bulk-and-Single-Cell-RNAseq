{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended RNA-Seq Analysis Training Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to analyze a complete bulk RNA-Seq analysis zebrafish (*Danio rerio*) data set using the complete set of reads from the NCBI Sequence Read Archive and the entire genome assembly. Steps in the workflow include read trimming, read QC, read mapping, and counting mapped reads per gene to estimate gene expression. The tutorial also includes some extra steps, such as how to access SRA metadata using Athena, a more powerful method for retrieving reads compared to sra-tools. This tutorial workflow uses a subset of sequence reads from a study published by Hartig et al (PubMed ID: [2746894](https://pubmed.ncbi.nlm.nih.gov/32746894/); Gene Expression Omnibus: [GSE137987](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE137987)). In this study, the authors were interested in examining how chronic early life stress alters gene expression in adulthood. RNA samples were extracted from the blood of adult zebrafish who were exposed to cortisol or control conditions during just the first 5 days of development. There were three control (DMSO-treated) samples, and three cortisol-treated samples.\n",
    "\n",
    "WARNING: This tutorial will take over 13 hours to run using ml.m5.4xlarge instance. The shorter version of this tutorial only analyzes a subset of the read data (1 million reads per sample) from this experiment and aligns them to just chromosome 14 rather than the whole genome because analyzing the entire data set takes much longer to run.\n",
    "\n",
    "The read counts generated by this tutorial are the same as what is used in the R-based differential gene analysis workflow tutorial (Tutorial 2).\n",
    "\n",
    "![RNA-Seq workflow](../../images/Tutorial_1_Workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Install Miniforge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install Miniforge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Miniforge or Mambaforge (you can use either based on preference)\n",
    "!curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\n",
    "\n",
    "# Install Miniforge (or Mambaforge) - no need to install conda since mamba will be available immediately\n",
    "!bash Miniforge3-$(uname)-$(uname -m).sh -b -u -p $HOME/miniforge > /dev/null\n",
    "!date +\"%T\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using mambaforge and bioconda, install the tools that will be used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update PATH to point to the Miniforge (or Mambaforge) bin files\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"HOME\"]+\"/miniforge/bin\"\n",
    "\n",
    "#now we can easily use 'mamba' command to install software \n",
    "!mamba install -y -c conda-forge -c bioconda trimmomatic fastqc multiqc sql-magic entrez-direct gffread parallel-fastq-dump sra-tools sql-magic pyathena samtools star rsem entrez-direct subread pigz -y > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of directories in the sra-data-athena to store the reads, reference sequence files, and output files. Notice that first we remove the `data` directory to clean up files from Tutorial_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $HOMEDIR\n",
    "!echo $PWD\n",
    "!mkdir -p data\n",
    "!mkdir -p data/trunc_rawfastq\n",
    "!mkdir -p data/trimmed\n",
    "!mkdir -p data/fastqc/\n",
    "!mkdir -p data/fastqc_samples/\n",
    "!mkdir -p data/reference\n",
    "!mkdir -p data/aligned_bam\n",
    "!mkdir -p data/rsem_reference/zebrafish_rsem_reference\n",
    "!mkdir -p data/rsem_output\n",
    "!mkdir -p data/reference/STAR_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set # THREADS depending on your VM size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "THREADS = max(1, num_cores - 1)\n",
    "\n",
    "print(\"Number of threads:\", THREADS)\n",
    "os.environ[\"THREADS\"] = str(THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Downloading relevant FASTQ files using SRA Tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need to download the relevant fastq files.\n",
    "\n",
    "Because these files can be large, the process of downloading and extracting fastq files can be quite lengthy.\n",
    "\n",
    "We will be downloading the sample runs from this project using SRA tools, downloading from the NCBI's SRA (Sequence Run Archives).\n",
    "\n",
    "However, first we need to find the associated accession numbers in order to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1: Finding run accession numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SRA stores sequence data in terms of runs, (SRR stands for Sequence Read Run). To download runs, we will need the accession ID for each run we wish to download.\n",
    "\n",
    "The MittenbÃ¼hler MJ et al., project contains 8 runs. To make it easier, these are the run IDs associated with this project:\n",
    "\n",
    "- SRR3371417\n",
    "- SRR3371418\n",
    "- SRR3371419\n",
    "- SRR3371420\n",
    "- SRR3371421\n",
    "- SRR3371422\n",
    "\n",
    "In this case, all these runs belong to the Bioproject PRJNA318296.\n",
    "\n",
    "Sequence run experiments can be searched for using the SRA database on the NCBI website; and article-specific sample run information can be found in the supplementary section of that article.\n",
    "\n",
    "Once the accession numbers are located, one can make a text file containing the list of accession IDs however they like.\n",
    "\n",
    "Once again, to make things easier, we have made a .txt with these IDs that you can simply download here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!esearch -db sra -query \"PRJNA318296\" | efetch -format runinfo | cut -d',' -f1 | tail -n +2 > accs.txt\n",
    "!cat accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.2 Finding run accession numbers using Athena (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athena is a serverless query engine that allows you to analyze data stored in Amazon S3 using standard SQL. It offers several advantages over traditional SRA tools, making it a more efficient and scalable solution for large-scale RNA-seq data analysis.\n",
    "\n",
    "Using Athena to access metadata is optional, but allows you to query large SRA metadata directly from AWS without needing to download and process files locally, making it faster and more scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "# Use the correct argument name: s3_staging_dir\n",
    "conn = connect(s3_staging_dir='s3://sra-data-athena/', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Glue client\n",
    "glue_client = boto3.client('glue', region_name='us-east-1')\n",
    "\n",
    "# Run the crawler\n",
    "crawler_name = 'sra_crawler'  # Use your crawler's name\n",
    "glue_client.start_crawler(Name=crawler_name)\n",
    "\n",
    "print(f\"Crawler {crawler_name} started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM AwsDataCatalog.srametadata.metadata\n",
    "WHERE bioproject = 'PRJNA318296'\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the SRR column to a text file\n",
    "with open('accs.txt', 'w') as f:\n",
    "    accs = df['acc'].to_string(header=False, index=False)\n",
    "    f.write(accs)\n",
    "    \n",
    "#print the text file\n",
    "!cat accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.3 Using the SRA-toolkit for a single sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet demonstrates how to download and preprocess single SRA data using the prefetch and fasterq-dump commands. First, prefetch downloads the specified SRA file and then, fasterq-dump converts the SRA file into paired-end FASTQ files. Finally, the generated FASTQ files are compressed using pigz to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for SRA download:\n",
    "!prefetch SRR3371420 -O data/raw_fastq -f yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sra to fastq\n",
    "!fasterq-dump data/raw_fastq/SRR3371420 -f -O data/raw_fastq/ -e $THREADS\n",
    "#compress fastq to fastq.gz to save space\n",
    "!pigz -p $THREADS data/raw_fastq/SRR3371420_1.fastq\n",
    "!pigz -p $THREADS data/raw_fastq/SRR3371420_2.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.4 Downloading multiple files using the SRA-toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses prefetch to download multiple SRA files in parallel. It reads the list of SRR IDs from accs.txt, uses xargs to execute prefetch for each ID, and specifies the output directory and the -f option to create FASTQ files in the same directory as the SRA files. To speed up the download the code uses -P $THREADS option allowing parallel execution using the specified number of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat accs.txt | xargs -P $THREADS -I {} prefetch {} -O data/raw_fastq -f yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.5 Converting Multiple SRA files to Fastq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the SRA files will be processed in parallel using parallel-fastq-dump. Each SRR ID from accs.txt will be read, and xargs will be used to execute parallel-fastq-dump for each SRA ID. This will result in the creation of two paired-end FASTQ files for each SRR ID, which will be compressed into a .gz file to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!for x in `cat accs.txt`; do fasterq-dump -f -O data/raw_fastq -e $THREADS -m 4G data/raw_fastq/$x/$x.sra; done\n",
    "\n",
    "##example of how to alternatively do the above process with parallel-fastq-dump using piping\n",
    "!cat accs.txt | xargs -I {} parallel-fastq-dump -O data/raw_fastq/ --tmpdir . --threads $THREADS --gzip --split-files --sra-id {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, it is good practice to turn .fastq files into .fastq.gz files to save space.\n",
    "\n",
    "In our case, we will actually need to concatenate the fastq files later on, and so will zip after this.\n",
    "\n",
    "The no redundant SRA files can also be deleted to save more space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find and delete all SRR subfolders in the raw_fastq directory\n",
    "!find data/raw_fastq -type d -name 'SRR*' -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.6 Download reference transcriptome files that will be used by STAR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step downloads and prepares the reference data needed for your RNA-seq analysis. It retrieves three essential files:\n",
    "\n",
    "- Zebrafish genome (Danio_rerio.GRCz11.dna.primary_assembly.fa.gz): This compressed FASTA file contains the complete Zebrafish genome sequence, that will be used as the reference for aligning your RNA-seq reads.\n",
    "- Zebrafish gene annotations (Danio_rerio.GRCz11.104.gtf.gz): This compressed GTF file provides information about the genes and transcripts in the Zebrafish genome, including their locations and structures. This data will crucial for interpreting the aligned RNA-seq reads and understanding what genes are expressed in each.\n",
    "- Zebrafish feature table (GCF_000002035.6_GRCz11_feature_table.txt.gz): This compressed table provides additional annotations for the Zebrafish genome features, potentially including information about gene functions and pathways. This step will further used to analyze the differential gene expression (DEG) analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget ftp://ftp.ensembl.org/pub/release-104/fasta/danio_rerio/dna/Danio_rerio.GRCz11.dna.primary_assembly.fa.gz -O data/reference/zebrafish_genome.fa.gz\n",
    "! wget ftp://ftp.ensembl.org/pub/release-104/gtf/danio_rerio/Danio_rerio.GRCz11.104.gtf.gz -O data/reference/zebrafish_annotation.gtf.gz\n",
    "! wget -O data/reference/zebrafish_feature_table.txt.gz https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/035/GCF_000002035.6_GRCz11/GCF_000002035.6_GRCz11_feature_table.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -f data/reference/zebrafish_genome.fa.gz \n",
    "!gunzip -f data/reference/zebrafish_annotation.gtf.gz\n",
    "!gunzip -f data/reference/zebrafish_feature_table.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.7: Copy data file for Trimmomatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of trimmomatics functions is to trim sequence machine specific adapter sequences. These are usually within the trimmomatic installation directory in a folder called adapters.\n",
    "\n",
    "Directories of packages within conda installations can be confusing, so in the case of using conda with trimmomatic, it may be easier to simply download or create a file with the relevant adapter sequencecs and store it in an easy to find directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P data/trimmed/ https://sra-data-athena.s3.amazonaws.com/reference/TruSeq3-PE.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Run FastQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastQC is an invaluable tool that allows you to evaluate whether there are problems with a set of reads. For example, it will provide a report of whether there is any bias in the sequence composition of the reads.\n",
    "\n",
    "The below code may take a while to run. To make it run faster we can use threads to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fastqc for forward reads in parallel\n",
    "!cat accs.txt | xargs -P $THREADS -I {} fastqc \"data/raw_fastq/{}_1.fastq.gz\" -o data/fastqc/\n",
    "\n",
    "# Run fastqc for reverse reads in parallel\n",
    "!cat accs.txt | xargs -P $THREADS -I {} fastqc \"data/raw_fastq/{}_2.fastq.gz\" -o data/fastqc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastqc will output the results in HTML format, as below, for all forward and reverse reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='./data/fastqc/SRR3371420_1_fastqc.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although its best practice to look over them individually, tools like multiqc allow one to quickly look at a summary of the quality reports of the fastq files.\n",
    "\n",
    "For instance, the below table shows which warnings, passes, or failures, from each fastqc report. There are other summaries created as well by multiqc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!multiqc -f data/fastqc/\n",
    "\n",
    "import pandas as pd\n",
    "dframe = pd.read_csv(\"./multiqc_data/multiqc_fastqc.txt\", sep='\\t')\n",
    "display(dframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Merging our fastq files (Optional if there are multiple SRR per GSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the project used presents the multiple SRAs per GSM we can use Athena to access the SRA metadata and merging the FASTQ files, the code simplifies the subsequent analysis steps and reduces the number of files to process. This can improve efficiency and reduce computational overhead.\n",
    "In this study this step was not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "# Use the correct argument name: s3_staging_dir\n",
    "conn = connect(s3_staging_dir='s3://sra-data-athena/', region_name='us-east-1')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM AwsDataCatalog.srametadata.metadata\n",
    "WHERE bioproject = 'PRJNAXXXXXXX' #Change to the Bioproject number\n",
    "AND organism = 'Danio rerio'\n",
    "\"\"\"\n",
    "df = pd.read_sql(\n",
    "    query, conn\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os so we can easily pass strings to shell commands using 'subprocess'\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "#now get the accession id's and sample id's from the created dataframe\n",
    "runs = df['acc'].values\n",
    "samples = list(set(df['acc'].values))\n",
    "\n",
    "#sort them to be in numerical order\n",
    "runs.sort()\n",
    "samples.sort()\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now iterate through the samples, \n",
    "#because there are two SRRs to a run, \n",
    "#this means corresponding SRRs indices to an index of a GSM will be\n",
    "#gsm index *2, and *2+1 \n",
    "for index, item in enumerate(samples):\n",
    "    \n",
    "    #concatenate the two SRRs\n",
    "    os.system(f\"cat data/raw_fastq/{runs[index*2]}_1.fastq data/raw_fastq/{runs[index*2+1]}_1.fastq > data/raw_fastq/{samples[index]}_1.fastq\")\n",
    "    #delete the previous fastq files to save space\n",
    "    os.system(f\"rm data/raw_fastq/{runs[index*2]}_1.fastq\")\n",
    "    os.system(f\"rm data/raw_fastq/{runs[index*2+1]}_1.fastq\")\n",
    "    #zip the merged fastq file to save more space\n",
    "    os.system(f\"gzip data/raw_fastq/{samples[index]}_1.fastq\")\n",
    "    \n",
    "    #repeat for reverse reads\n",
    "    os.system(f\"cat data/raw_fastq/{runs[index*2]}_2.fastq data/raw_fastq/{runs[index*2+1]}_2.fastq > data/raw_fastq/{samples[index]}_2.fastq\")\n",
    "    \n",
    "    os.system(f\"rm data/raw_fastq/{runs[index*2]}_2.fastq\")\n",
    "    os.system(f\"rm data/raw_fastq/{runs[index*2+1]}_2.fastq\")  \n",
    "   \n",
    "    #its good practice to zip files to save space\n",
    "    os.system(f\"gzip data/raw_fastq/{samples[index]}_2.fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since our files will now be samples, not SRRs we can write a new text file to use for downstream batch processes.\n",
    "#we can use the DF we made in the previous cell.\n",
    "with open('samples.txt', 'w') as f:\n",
    "    df = df.sort_values(by='sample_name', ascending=True)\n",
    "    samples = df['acc'].unique()\n",
    "    samples = '\\n'.join(map(str, samples))\n",
    "    f.write(samples)\n",
    "    \n",
    "!cat samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Run Trimmomatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trimmomatic will trim off any adapter sequences or low quality sequence it detects in the FASTQ files.\n",
    "\n",
    "Using piping and our original list, it is possible to queue up a batch run of trimmomatic for all our files, note that this is a different way to run a loop compared with what we did before.\n",
    "\n",
    "The below code may take approximately 30 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat accs.txt | xargs -I {} \\\n",
    "trimmomatic PE -threads $THREADS \\\n",
    "'data/raw_fastq/{}_1.fastq.gz' 'data/raw_fastq/{}_2.fastq.gz' \\\n",
    "'data/trimmed/{}_1_trimmed.fastq' 'data/trimmed/{}_1_trimmed_unpaired.fastq' \\\n",
    "'data/trimmed/{}_2_trimmed.fastq' 'data/trimmed/{}_2_trimmed_unpaired.fastq' \\\n",
    "ILLUMINACLIP:data/trimmed/TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Run FastQC\n",
    "It's best practice to run FastQC after trimming. However, you may decide to run FastQC only once, before or after trimming.\n",
    "\n",
    "We will proceed with only the forward reads -- this is because, looking at trimmomatic, there were very few 'orphaned' reads. That is to say, most forward and reverse reads were successfully paired together. Because we are just trying to map to a transcriptome, the read lengths of the forward reads alone, in this case, around 60 millions~ basepairs, should be sufficient.\n",
    "\n",
    "The below code may take around 15-20 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FastQC\n",
    "!cat accs.txt | xargs -P $THREADS -I {} fastqc data/trimmed/{}_1_trimmed.fastq data/trimmed/{}_2_trimmed.fastq -o data/fastqc_samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: Run MultiQC\n",
    "MultiQC reads in the FastQC reports and generate a compiled report for all the analyzed FASTQ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!multiqc -f data/fastqc_samples/\n",
    "!multiqc -f -o data/multiqc_samples/ data/fastqc_samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: Preparing the STAR-Compatible RSEM Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command prepares a reference genome and annotation files for RNA-Seq analysis using RSEM (RNA-Seq by Expectation-Maximization) and STAR (Spliced Transcripts Alignment to a Reference). It generates files needed to quantify gene and isoform expression. The rsem-prepare-reference function takes a GTF file with gene annotations (zebrafish_annotation.gtf) and a FASTA file with the reference genome sequence (zebrafish_genome.fa). It processes these files to create a reference, saving the output in the zebrafish_reference directory. The --star option ensures the reference is compatible with STAR for efficient transcriptome alignment. The -p $THREADS option sets the number of threads used for parallel processing, speeding up the preparation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the reference genome\n",
    "!rsem-prepare-reference --gtf data/reference/zebrafish_annotation.gtf --star -p $THREADS data/reference/zebrafish_genome.fa zebrafish_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: Run STAR for Alignment, Prepare and Run RSEM for Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script automates RNA-Seq gene expression quantification using RSEM and STAR. It reads SRR accession IDs from accs.txt, saves results in data/rsem_output, and runs rsem-calculate-expression for each ID. It uses paired-end trimmed FASTQ files from data/trimmed/ and a STAR-aligned RSEM reference (zebrafish_reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure you've set the path to the RSEM binary\n",
    "# Read the SRR accessions from the file\n",
    "with open('accs.txt', 'r') as f:\n",
    "    srr_accessions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"data/rsem_output\"\n",
    "\n",
    "# Loop through each SRR accession and run rsem-calculate-expression\n",
    "for srr in srr_accessions:\n",
    "    !rsem-calculate-expression -p $THREADS --paired-end --star \\\n",
    "    data/trimmed/{srr}_1_trimmed.fastq data/trimmed/{srr}_2_trimmed.fastq zebrafish_reference data/rsem_output/{srr} > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: Report the top 10 most highly expressed genes in the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in each wild-type sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to RSEM results directory\n",
    "rsem_results_dir = 'data/rsem_output'\n",
    "\n",
    "# Loop through each file in accs.txt\n",
    "for srr_id in open('accs.txt'):\n",
    "    srr_id = srr_id.strip()  # Remove newline character\n",
    "    rsem_result_file = f'{rsem_results_dir}/{srr_id}.genes.results'\n",
    "\n",
    "    # Load the RSEM results into a Pandas DataFrame\n",
    "    df = pd.read_csv(rsem_result_file, sep='\\t')\n",
    "\n",
    "    # Sort the DataFrame by TPM values in descending order and get the top 10 genes\n",
    "    top_10_genes = df.sort_values(by='TPM', ascending=False).head(10)\n",
    "\n",
    "    # Print the top 10 genes with their TPM values\n",
    "    print(f\"Top 10 Genes by TPM for {srr_id}:\")\n",
    "    print(top_10_genes[['gene_id', 'TPM']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12: Report the expression of irg1l for each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the wild-type sample. The fields in the RSEM `genes.results` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to RSEM results directory\n",
    "rsem_results_dir = 'data/rsem_output'\n",
    "\n",
    "# Target gene ID\n",
    "target_gene = 'ENSDARG00000062788'\n",
    "\n",
    "# Loop through each file in accs.txt\n",
    "for srr_id in open('accs.txt'):\n",
    "    srr_id = srr_id.strip()  # Remove newline character\n",
    "    rsem_result_file = f'{rsem_results_dir}/{srr_id}.genes.results'\n",
    "\n",
    "    # Load the RSEM results into a Pandas DataFrame\n",
    "    df = pd.read_csv(rsem_result_file, sep='\\t')\n",
    "\n",
    "    # Filter for the target gene\n",
    "    target_gene_data = df[df['gene_id'] == target_gene]\n",
    "\n",
    "    # Print the target gene's TPM value for the SRR ID\n",
    "    print(f\"TPM for {target_gene} in {srr_id}: {target_gene_data['TPM'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 13: Export Read counts to S3 Bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code effectively extracts gene expression data from RSEM output files and stores them in a structured format on an S3 bucket. This data will be accessible for further analysis in Tutorial 2 and Tutorial 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# Define the path to your RSEM output directory\n",
    "rsem_output_path = \"data/rsem_output\"\n",
    "\n",
    "# Define the S3 bucket and output path\n",
    "s3_bucket = \"sra-data-athena\"\n",
    "s3_output_path = \"readcounts/\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Get a list of all .genes.results files in the directory\n",
    "genes_files = [f for f in os.listdir(rsem_output_path) if f.endswith('.genes.results')]\n",
    "\n",
    "# Loop through each file to extract gene ID, expected counts, and gene length\n",
    "for file in genes_files:\n",
    "    file_path = os.path.join(rsem_output_path, file)\n",
    "    \n",
    "    # Read the .genes.results file\n",
    "    rsem_data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "    # Check if the necessary columns exist\n",
    "    if all(col in rsem_data.columns for col in [\"gene_id\", \"expected_count\", \"length\"]):\n",
    "        # Create a new dataframe with required columns\n",
    "        result_data = rsem_data[[\"gene_id\", \"expected_count\", \"length\"]]\n",
    "        result_data.columns = [\"GeneID\", \"Count\", \"GeneLength\"]\n",
    "\n",
    "        # Define the output filename based on the input file name\n",
    "        output_file_name = f\"{os.path.splitext(file)[0]}_zebrafish.txt\"\n",
    "        s3_output_file_path = f\"{s3_output_path}{output_file_name}\"\n",
    "\n",
    "        # Convert the DataFrame to a CSV string\n",
    "        csv_buffer = result_data.to_csv(sep=\"\\t\", index=False)\n",
    "\n",
    "        # Upload the result directly to S3\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=s3_output_file_path, Body=csv_buffer)\n",
    "\n",
    "    else:\n",
    "        print(f\"Warning: Required columns are missing in file: {file}\")\n",
    "\n",
    "# Optionally, print a message indicating completion\n",
    "print(\"Extraction and file creation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 14: Save Merged Read Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code combines multiple RSEM gene count files into a single, unified file, making it easier to analyze and visualize the gene expression data. This files was also uploaded to S3 Bucket to allow further analysis in other Tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the RSEM quantification results directory exists\n",
    "!mkdir -p data/rsem_output\n",
    "\n",
    "# Merge RSEM results by gene counts (similar to Salmon's numreads merge)\n",
    "!rsem-generate-data-matrix data/rsem_output/*.genes.results > data/rsem_output/merged_gene_counts_zebrafish.txt\n",
    "\n",
    "# Optionally, rename the columns based on the samples\n",
    "# If you want to assign your GSM identifiers or any other custom names, edit the header.\n",
    "!sed -i \"1s/.*/Name\\tGSM2121180\\tGSM2121181\\tGSM2121182\\tGSM2121183\\tGSM2121184\\tGSM2121185/\" data/rsem_output/merged_gene_counts_zebrafish.txt\n",
    "\n",
    "# Remove any unnecessary prefixes like 'gene-' or 'rna-' for easier formatting\n",
    "!sed -i \"s/gene-//g\" data/rsem_output/merged_gene_counts_zebrafish.txt\n",
    "!sed -i \"s/rna-//g\" data/rsem_output/merged_gene_counts_zebrafish.txt\n",
    "\n",
    "# Show a preview of the merged quantification file\n",
    "!head data/rsem_output/merged_gene_counts_zebrafish.txt\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Define the file path and S3 bucket details\n",
    "file_path = \"data/rsem_output/merged_gene_counts_zebrafish.txt\"\n",
    "bucket_name = \"sra-data-athena\"\n",
    "s3_key = \"readcounts/merged_gene_counts_zebrafish.txt\"\n",
    "\n",
    "# Initialize an S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Upload the file to the specified S3 bucket\n",
    "try:\n",
    "    s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "    print(f\"File {file_path} uploaded successfully to {bucket_name}/{s3_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")\n",
    "\n",
    "# Define the file paths and S3 bucket details\n",
    "rsem_output_path = \"data/rsem_output\"\n",
    "feature_table_path = \"data/reference/zebrafish_feature_table.txt\"\n",
    "bucket_name = \"sra-data-athena\"\n",
    "s3_output_path = \"readcounts/\"\n",
    "s3_feature_table_path = \"reference/zebrafish_feature_table.txt\"\n",
    "\n",
    "# ... (rest of the code remains the same)\n",
    "\n",
    "# Upload the gene count file\n",
    "s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "\n",
    "# Upload the feature table file\n",
    "s3_client.upload_file(feature_table_path, bucket_name, s3_feature_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"workflow\">Additional Workflows</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have read counts per gene, feel free to explore the R workflow which creates plots and analyses using these readcount files, or try other alternate workflows for creating read count files, such as using snakemake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Workflow One:](Tutorial_1_subsampling_zebrafish-miniforge.ipynb) A short introduction to downloading and mapping sequences to a zebrafish genome using STAR and RSEM.\n",
    "\n",
    "\n",
    "[Workflow Two (DEG Analysis):](Tutorial_2_DEG_Analysis_zebrafish.ipynb) Using Deseq2 and R to conduct clustering and differential gene expression analysis."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
