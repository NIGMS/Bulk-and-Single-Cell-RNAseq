{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended RNA-Seq Analysis for Zebrafish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> <b>WARNING</b>: This tutorial will take over <u>13 hours</u> to run using an <b>ml.m5.4xlarge</b> instance. The shorter version of this tutorial (Tutorial_1) only analyzes a subset of the read data (1 million reads per sample) from this experiment and aligns them to just chromosome 14 rather than the whole genome because analyzing the entire data set takes much longer to run.</div>\n",
    "\n",
    "This tutorial demonstrates how to analyze a complete bulk RNA-Seq analysis zebrafish (*Danio rerio*) data set using the complete set of reads from the NCBI Sequence Read Archive and the entire genome assembly. Steps in the workflow include read trimming, read QC, read mapping, and counting mapped reads per gene to estimate gene expression. This tutorial workflow uses a subset of sequence reads from a study published by Hartig et al (PubMed ID: [2746894](https://pubmed.ncbi.nlm.nih.gov/32746894/); Gene Expression Omnibus: [GSE137987](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE137987)). In this study, the authors were interested in examining how chronic early life stress alters gene expression in adulthood. RNA samples were extracted from the blood of adult zebrafish who were exposed to cortisol or control conditions during just the first 5 days of development. There were three control (DMSO-treated) samples, and three cortisol-treated samples.\n",
    "\n",
    "The read counts generated by this tutorial are the same as what is used in the R-based differential gene analysis workflow tutorial (Tutorial 2).\n",
    "\n",
    "![RNA-Seq workflow](../../images/Tutorial_1_Workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> NOTE: This Jupyter Notebook was developed to run within a customized container on AWS with all software and tools pre-configured. If running without this customized container, you will need to install tools using the Miniforge environment setup instructions below before moving on to Step 2.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Container: Install Miniforge and Workflow Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miniforge is a lightweight Conda distribution that offers a streamlined installation process and efficient package management. It provides access to a vast repository of packages.\n",
    "\n",
    "Conda packages and environments are useful for several reasons. Conda packages contain metadata. This metadata includes information about what other programs the given software needs to be installed, in order to run. When installing a package with Conda, those other packages are automatically also installed. In this way, the user does not have to worry about manually installing each dependency. This makes installation quick and simple.\n",
    "\n",
    "These packages are installed inside of environments, which are simply folders within the local installation of Conda. This has several benefits. Local installation means easier installation for non-admin users who may not have access to all system directories. Each environment can hold specific software with specific versions, and it easy to swap to different environments. In addition, the environments themselves are portable, as each environment contains a manifest on how to recreate that environment.\n",
    "\n",
    "Miniforge itself is a Conda package manager, this means it requires Conda in order to work. It is used to install and update Conda packages, which it gets from a ‘channel’, or repository. It is an alternative to the native Conda package manager. It is often used for reasons of speed.\n",
    "\n",
    "Bioconda is a ‘channel’, or repository, that the Mambaforge package manager can download packages from. It is a repository of Conda packages that are related to biology. These packages are versions of popular biology software that are curated and uploaded by contributing users.\n",
    "\n",
    "The following code performs these steps:\n",
    "- Downloads Miniforge or Mambaforge (you can use either based on preference)\n",
    "- Installs Miniforge (or Mambaforge) - no need to install conda since mamba will be available immediately\n",
    "- Using miniforge and bioconda, installs the tools that will be used in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Tip: If using the Miniforge install, run the following code cells by removing the %%script false command. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Download Miniforge or Mambaforge (you can use either based on preference)\n",
    "!curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\n",
    "\n",
    "# Install Miniforge (or Mambaforge) - no need to install conda since mamba will be available immediately\n",
    "!bash Miniforge3-$(uname)-$(uname -m).sh -b -u -p $HOME/miniforge > /dev/null\n",
    "!date +\"%T\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using mambaforge and bioconda, install the tools that will be used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Update PATH to point to the Miniforge (or Mambaforge) bin files\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"HOME\"]+\"/miniforge/bin\"\n",
    "\n",
    "#now we can easily use 'mamba' command to install software \n",
    "!mamba install -y -c conda-forge -c bioconda trimmomatic fastqc multiqc sql-magic entrez-direct gffread parallel-fastq-dump sra-tools sql-magic pyathena samtools star rsem entrez-direct subread pigz -y > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "## If running from a container, as noted above, start with <b> STEP 2 </b> below:\n",
    "## STEP 2: Define Threads & Setup Directory Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of directories in the sra-data-athena to store the reads, reference sequence files, and output files. Notice that first we remove the `data` directory to clean up files from Tutorial_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $HOMEDIR\n",
    "!echo $PWD\n",
    "!mkdir -p data\n",
    "!mkdir -p data/trunc_rawfastq\n",
    "!mkdir -p data/trimmed\n",
    "!mkdir -p data/fastqc\n",
    "!mkdir -p data/fastqc_trimmed\n",
    "!mkdir -p data/reference\n",
    "!mkdir -p data/aligned_bam\n",
    "!mkdir -p data/rsem_reference/zebrafish_rsem_reference\n",
    "!mkdir -p data/rsem_output\n",
    "!mkdir -p data/reference/STAR_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set # THREADS depending on your VM size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "THREADS = max(1, num_cores - 1)\n",
    "\n",
    "print(\"Number of threads:\", THREADS)\n",
    "os.environ[\"THREADS\"] = str(THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Downloading relevant FASTQ files using SRA Tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need to download the relevant fastq files. Because these files can be large, the process of downloading and extracting fastq files can be quite lengthy. We will be downloading the sample runs from this project using SRA tools, downloading from the NCBI's SRA (Sequence Run Archives). However, first we need to find the associated accession numbers in order to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1: Finding run accession numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SRA stores sequence data in terms of runs, (SRR stands for Sequence Read Run). To download runs, we will need the accession ID for each run we wish to download.\n",
    "\n",
    "The <i>Mittenbühler MJ et al.</i>, project contains 8 runs. To make it easier, these are the run IDs associated with this project:\n",
    "\n",
    "- `SRR3371417`\n",
    "- `SRR3371418`\n",
    "- `SRR3371419`\n",
    "- `SRR3371420`\n",
    "- `SRR3371421`\n",
    "- `SRR3371422`\n",
    "\n",
    "In this case, all these runs belong to the Bioproject <b>PRJNA318296</b>. Sequence run experiments can be searched for using the SRA database on the NCBI website; and article-specific sample run information can be found in the supplementary section of that article. Once the accession numbers are located, one can make a text file containing the list of accession IDs however they like. Once again, to make things easier, we have made a .txt with these IDs that you can simply download here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!esearch -db sra -query \"PRJNA318296\" | efetch -format runinfo | cut -d',' -f1 | tail -n +2 > accs.txt\n",
    "!cat accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.2 Downloading multiple files using the SRA-toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses prefetch to download multiple SRA files in parallel. It reads the list of SRR IDs from accs.txt, uses xargs to execute prefetch for each ID, and specifies the output directory and the -f option to create FASTQ files in the same directory as the SRA files. To speed up the download the code uses -P $THREADS option allowing parallel execution using the specified number of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat accs.txt | xargs -P $THREADS -I {} prefetch {} -O data/raw_fastq -f yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.3 Converting Multiple SRA files to Fastq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the SRA files will be processed in parallel using parallel-fastq-dump. Each SRR ID from accs.txt will be read, and xargs will be used to execute parallel-fastq-dump for each SRA ID. This will result in the creation of two paired-end FASTQ files for each SRR ID, which will be compressed into a .gz file to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process with parallel-fastq-dump using piping\n",
    "!cat accs.txt | xargs -I {} parallel-fastq-dump -O data/raw_fastq/ --tmpdir . --threads $THREADS --gzip --split-files --sra-id {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, it is good practice to turn .fastq files into .fastq.gz files to save space. In our case, we will actually need to concatenate the fastq files later on, and so will zip after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find and delete all SRR subfolders in the raw_fastq directory\n",
    "!find data/raw_fastq -type d -name 'SRR*' -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.4 Download reference transcriptome files that will be used by STAR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step downloads and prepares the reference data needed for your RNA-seq analysis. It retrieves three essential files:\n",
    "\n",
    "- Zebrafish genome (Danio_rerio.GRCz11.dna.primary_assembly.fa.gz): This compressed FASTA file contains the complete Zebrafish genome sequence, that will be used as the reference for aligning your RNA-seq reads.\n",
    "- Zebrafish gene annotations (Danio_rerio.GRCz11.104.gtf.gz): This compressed GTF file provides information about the genes and transcripts in the Zebrafish genome, including their locations and structures. This data will crucial for interpreting the aligned RNA-seq reads and understanding what genes are expressed in each.\n",
    "- Zebrafish feature table (GCF_000002035.6_GRCz11_feature_table.txt.gz): This compressed table provides additional annotations for the Zebrafish genome features, potentially including information about gene functions and pathways. This step will further used to analyze the differential gene expression (DEG) analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget ftp://ftp.ensembl.org/pub/release-104/fasta/danio_rerio/dna/Danio_rerio.GRCz11.dna.primary_assembly.fa.gz -O data/reference/zebrafish_genome.fa.gz\n",
    "!wget ftp://ftp.ensembl.org/pub/release-104/gtf/danio_rerio/Danio_rerio.GRCz11.104.gtf.gz -O data/reference/zebrafish_annotation.gtf.gz\n",
    "!wget -O data/reference/zebrafish_feature_table.txt.gz https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/035/GCF_000002035.6_GRCz11/GCF_000002035.6_GRCz11_feature_table.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -f data/reference/zebrafish_genome.fa.gz \n",
    "!gunzip -f data/reference/zebrafish_annotation.gtf.gz\n",
    "!gunzip -f data/reference/zebrafish_feature_table.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.5 Copy data file for Trimmomatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of trimmomatics functions is to trim sequence machine specific adapter sequences. These are usually within the trimmomatic installation directory in a folder called adapters.\n",
    "\n",
    "Directories of packages within conda installations can be confusing, so in the case of using conda with trimmomatic, it may be easier to simply download or create a file with the relevant adapter sequencecs and store it in an easy to find directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P data/trimmed/ https://nigms-sandbox.s3.us-east-1.amazonaws.com/bulk-scRNAseq/reference/TruSeq3-PE.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Run FastQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastQC is an invaluable tool that allows you to evaluate whether there are problems with a set of reads. For example, it will provide a report of whether there is any bias in the sequence composition of the reads. The below code may take a while to run. To make it run faster we can use threads to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fastqc for forward reads in parallel\n",
    "!cat accs.txt | xargs -P $THREADS -I {} fastqc \"data/raw_fastq/{}_1.fastq.gz\" -o data/fastqc/\n",
    "\n",
    "# Run fastqc for reverse reads in parallel\n",
    "!cat accs.txt | xargs -P $THREADS -I {} fastqc \"data/raw_fastq/{}_2.fastq.gz\" -o data/fastqc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastqc will output the results in HTML format, as below, for all forward and reverse reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='data/fastqc/SRR3371420_1_fastqc.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although its best practice to look over them individually, tools like multiqc allow one to quickly look at a summary of the quality reports of the fastq files.\n",
    "\n",
    "For instance, the below table shows which warnings, passes, or failures, from each fastqc report. There are other summaries created as well by multiqc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!multiqc -f data/fastqc/\n",
    "\n",
    "import pandas as pd\n",
    "dframe = pd.read_csv(\"./multiqc_data/multiqc_fastqc.txt\", sep='\\t')\n",
    "display(dframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Run Trimmomatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trimmomatic will trim off any adapter sequences or low quality sequence it detects in the FASTQ files. Using piping and our original list, it is possible to queue up a batch run of trimmomatic for all our files, note that this is a different way to run a loop compared with what we did before. The below code may take approximately 30 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat accs.txt | xargs -I {} \\\n",
    "trimmomatic PE -threads $THREADS \\\n",
    "'data/raw_fastq/{}_1.fastq.gz' 'data/raw_fastq/{}_2.fastq.gz' \\\n",
    "'data/trimmed/{}_1_trimmed.fastq' 'data/trimmed/{}_1_trimmed_unpaired.fastq' \\\n",
    "'data/trimmed/{}_2_trimmed.fastq' 'data/trimmed/{}_2_trimmed_unpaired.fastq' \\\n",
    "ILLUMINACLIP:data/trimmed/TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Run FastQC\n",
    "It's best practice to run FastQC after trimming. However, you may decide to run FastQC only once, before or after trimming.\n",
    "\n",
    "We will proceed with only the forward reads -- this is because, looking at trimmomatic, there were very few 'orphaned' reads. That is to say, most forward and reverse reads were successfully paired together. Because we are just trying to map to a transcriptome, the read lengths of the forward reads alone, in this case, around 60 millions~ basepairs, should be sufficient.\n",
    "\n",
    "The below code may take around 15-20 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FastQC\n",
    "!cat accs.txt | xargs -P $THREADS -I {} fastqc data/trimmed/{}_1_trimmed.fastq data/trimmed/{}_2_trimmed.fastq -o data/fastqc_trimmed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Run MultiQC\n",
    "MultiQC reads in the FastQC reports and generate a compiled report for all the analyzed FASTQ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run multiqc\n",
    "!multiqc -f -o data/multiqc_samples/ data/fastqc_trimmed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: Preparing the STAR-Compatible RSEM Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command prepares a reference genome and annotation files for RNA-Seq analysis using RSEM (RNA-Seq by Expectation-Maximization) and STAR (Spliced Transcripts Alignment to a Reference). It generates files needed to quantify gene and isoform expression. The rsem-prepare-reference function takes a GTF file with gene annotations (zebrafish_annotation.gtf) and a FASTA file with the reference genome sequence (zebrafish_genome.fa). It processes these files to create a reference, saving the output in the zebrafish_reference directory. The --star option ensures the reference is compatible with STAR for efficient transcriptome alignment. The -p $THREADS option sets the number of threads used for parallel processing, speeding up the preparation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the reference genome\n",
    "!rsem-prepare-reference --gtf data/reference/zebrafish_annotation.gtf --star -p $THREADS data/reference/zebrafish_genome.fa zebrafish_reference > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: Run STAR for Alignment, Prepare and Run RSEM for Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script automates RNA-Seq gene expression quantification using RSEM and STAR. It reads SRR accession IDs from accs.txt, saves results in data/rsem_output, and runs rsem-calculate-expression for each ID. It uses paired-end trimmed FASTQ files from data/trimmed/ and a STAR-aligned RSEM reference (zebrafish_reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure you've set the path to the RSEM binary\n",
    "# Read the SRR accessions from the file\n",
    "with open('accs.txt', 'r') as f:\n",
    "    srr_accessions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"data/rsem_output\"\n",
    "\n",
    "# Loop through each SRR accession and run rsem-calculate-expression\n",
    "for srr in srr_accessions:\n",
    "    !rsem-calculate-expression -p $THREADS --paired-end --star \\\n",
    "    data/trimmed/{srr}_1_trimmed.fastq data/trimmed/{srr}_2_trimmed.fastq zebrafish_reference data/rsem_output/{srr} > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: Report the top 10 most highly expressed genes in the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in each wild-type sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to RSEM results directory\n",
    "rsem_results_dir = 'data/rsem_output'\n",
    "\n",
    "# Loop through each file in accs.txt\n",
    "for srr_id in open('accs.txt'):\n",
    "    srr_id = srr_id.strip()  # Remove newline character\n",
    "    rsem_result_file = f'{rsem_results_dir}/{srr_id}.genes.results'\n",
    "\n",
    "    # Load the RSEM results into a Pandas DataFrame\n",
    "    df = pd.read_csv(rsem_result_file, sep='\\t')\n",
    "\n",
    "    # Sort the DataFrame by TPM values in descending order and get the top 10 genes\n",
    "    top_10_genes = df.sort_values(by='TPM', ascending=False).head(10)\n",
    "\n",
    "    # Print the top 10 genes with their TPM values\n",
    "    print(f\"Top 10 Genes by TPM for {srr_id}:\")\n",
    "    print(top_10_genes[['gene_id', 'TPM']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: Report the expression of irg1l for each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the wild-type sample. The fields in the RSEM `genes.results` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "- `Name`\n",
    "- `Length`\n",
    "- `EffectiveLength`\n",
    "- `TPM`\n",
    "- `NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to RSEM results directory\n",
    "rsem_results_dir = 'data/rsem_output'\n",
    "\n",
    "# Target gene ID\n",
    "target_gene = 'ENSDARG00000062788'\n",
    "\n",
    "# Loop through each file in accs.txt\n",
    "for srr_id in open('accs.txt'):\n",
    "    srr_id = srr_id.strip()  # Remove newline character\n",
    "    rsem_result_file = f'{rsem_results_dir}/{srr_id}.genes.results'\n",
    "\n",
    "    # Load the RSEM results into a Pandas DataFrame\n",
    "    df = pd.read_csv(rsem_result_file, sep='\\t')\n",
    "\n",
    "    # Filter for the target gene\n",
    "    target_gene_data = df[df['gene_id'] == target_gene]\n",
    "\n",
    "    # Print the target gene's TPM value for the SRR ID\n",
    "    print(f\"TPM for {target_gene} in {srr_id}: {target_gene_data['TPM'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12: Export Read counts to S3 Bucket [OPTIONAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code effectively extracts gene expression data from RSEM output files and stores them in a structured format on an S3 bucket. This data will be accessible for further analysis in Tutorial 2 and Tutorial 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the S3 bucket, output path, and key\n",
    "s3_bucket_name = \"YOUR-BUCKET-NAME\" #EDIT to define your personal s3 bucket here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# Define the path to your RSEM output directory\n",
    "rsem_output_path = \"data/rsem_output\"\n",
    "\n",
    "# Define the S3 output path\n",
    "s3_output_path = \"bulk-scRNAseq/exported-readcounts/\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Get a list of all .genes.results files in the directory\n",
    "genes_files = [f for f in os.listdir(rsem_output_path) if f.endswith('.genes.results')]\n",
    "\n",
    "# Loop through each file to extract gene ID, expected counts, and gene length\n",
    "for file in genes_files:\n",
    "    file_path = os.path.join(rsem_output_path, file)\n",
    "    \n",
    "    # Read the .genes.results file\n",
    "    rsem_data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "    # Check if the necessary columns exist\n",
    "    if all(col in rsem_data.columns for col in [\"gene_id\", \"expected_count\", \"length\"]):\n",
    "        # Create a new dataframe with required columns\n",
    "        result_data = rsem_data[[\"gene_id\", \"expected_count\", \"length\"]]\n",
    "        result_data.columns = [\"GeneID\", \"Count\", \"GeneLength\"]\n",
    "\n",
    "        # Define the output filename based on the input file name\n",
    "        output_file_name = f\"{os.path.splitext(file)[0]}_zebrafish.txt\"\n",
    "        s3_output_file_path = f\"{s3_output_path}{output_file_name}\"\n",
    "\n",
    "        # Convert the DataFrame to a CSV string\n",
    "        csv_buffer = result_data.to_csv(sep=\"\\t\", index=False)\n",
    "\n",
    "        # Upload the result directly to S3\n",
    "        s3_client.put_object(Bucket=s3_bucket_name, Key=s3_output_file_path, Body=csv_buffer)\n",
    "\n",
    "    else:\n",
    "        print(f\"Warning: Required columns are missing in file: {file}\")\n",
    "\n",
    "# Optionally, print a message indicating completion\n",
    "print(\"Extraction and file creation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 13: Save Merged Read Counts [OPTIONAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code combines multiple RSEM gene count files into a single, unified file, making it easier to analyze and visualize the gene expression data. This files was also uploaded to S3 Bucket to allow further analysis in other Tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the RSEM quantification results directory exists\n",
    "!mkdir -p data/rsem_output\n",
    "\n",
    "# Merge RSEM results by gene counts (similar to Salmon's numreads merge)\n",
    "!rsem-generate-data-matrix data/rsem_output/*.genes.results > data/rsem_output/merged_gene_counts_zebrafish.txt\n",
    "\n",
    "# Optionally, rename the columns based on the samples\n",
    "# If you want to assign your GSM identifiers or any other custom names, edit the header.\n",
    "!sed -i \"1s/.*/Name\\tGSM2121180\\tGSM2121181\\tGSM2121182\\tGSM2121183\\tGSM2121184\\tGSM2121185/\" data/rsem_output/merged_gene_counts_zebrafish.txt\n",
    "\n",
    "# Remove any unnecessary prefixes like 'gene-' or 'rna-' for easier formatting\n",
    "!sed -i \"s/gene-//g\" data/rsem_output/merged_gene_counts_zebrafish.txt\n",
    "!sed -i \"s/rna-//g\" data/rsem_output/merged_gene_counts_zebrafish.txt\n",
    "\n",
    "# Show a preview of the merged quantification file\n",
    "!head data/rsem_output/merged_gene_counts_zebrafish.txt\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Define the file path and S3 bucket details\n",
    "file_path = \"data/rsem_output/merged_gene_counts_zebrafish.txt\"\n",
    "s3_key = \"bulk-scRNAseq/exported-readcounts/merged_gene_counts_zebrafish.txt\"\n",
    "\n",
    "# Initialize an S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Upload the file to the specified S3 bucket\n",
    "try:\n",
    "    s3_client.upload_file(file_path, s3_bucket_name, s3_key)\n",
    "    print(f\"File {file_path} uploaded successfully to {s3_bucket_name}/{s3_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")\n",
    "\n",
    "# Define the file paths and S3 bucket details\n",
    "rsem_output_path = \"data/rsem_output\"\n",
    "feature_table_path = \"data/reference/zebrafish_feature_table.txt\"\n",
    "s3_feature_table_path = \"bulk-scRNAseq/exported-reference/zebrafish_feature_table.txt\"\n",
    "\n",
    "# Upload the gene count file\n",
    "s3_client.upload_file(file_path, s3_bucket_name, s3_key)\n",
    "\n",
    "# Upload the feature table file\n",
    "s3_client.upload_file(feature_table_path, s3_bucket_name, s3_feature_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 14: Save RSEM reference and STAR index to AWS S3 Bucket [OPTIONAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 's3://....' #EDIT to define your personal s3 bucket here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If running, before copying to S3, make sure reference files are in correct directory\n",
    "!aws s3 cp data/rsem_reference $s3_bucket --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"workflow\">Additional Workflows</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have read counts per gene, feel free to explore the R workflow which creates plots and analyses using these readcount files, or try other alternate workflows for creating read count files, such as using snakemake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Workflow One:](Tutorial_1_subsampling_zebrafish-miniforge.ipynb) A short introduction to downloading and mapping sequences to a zebrafish genome using STAR and RSEM.\n",
    "\n",
    "\n",
    "[Workflow Two (DEG Analysis):](Tutorial_2_DEG_Analysis_zebrafish.ipynb) Using Deseq2 and R to conduct clustering and differential gene expression analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
