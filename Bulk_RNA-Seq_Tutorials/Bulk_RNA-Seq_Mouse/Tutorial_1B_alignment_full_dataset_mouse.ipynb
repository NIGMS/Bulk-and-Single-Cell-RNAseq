{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1b - Extended Bulk RNA-Seq Analysis for Mouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extended tutorial demonstrates how to run an RNA-Seq workflow using a full *Mus musculus* dataset. Steps in the workflow include read trimming, quality control, read mapping, and counting mapped reads per gene to quantitate gene expression. In addition to all steps in the short tutorial, this workflow includes accessing SRA metadata using Athena, a more powerful and faster step compared to sra-tools. This tutorial will analyze data from the Mittenbühler MJ et al. 2023, project.\n",
    "\n",
    "Mittenbühler MJ, Jedrychowski MP, Van Vranken JG, Sprenger HG, Wilensky S, Dumesic PA, Sun Y, Tartaglia A, Bogoslavski D, A M, Xiao H, Blackmore KA, Reddy A, Gygi SP, Chouchani ET, Spiegelman BM. Isolation of extracellular fluids reveals novel secreted bioactive proteins from muscle and fat tissues. Cell Metab. 2023;35(3):535-549.e7. [PMID: 36681077](https://pubmed.ncbi.nlm.nih.gov/36681077/).\n",
    "\n",
    "Full fastq files can be rather large, and the process of downloading, extracting, and analysis means this tutorial can take over 13 hours to run the code fully using an AWS ml.m5.4xlarge instance.\n",
    "\n",
    "All outputs used in [Tutorial 2](https://github.com/King-Laboratory/scRNASeq-miRNASeq-and-TF-Network-Analysis/blob/bda75860ace82cf180a6f9eae115ebaf2eabc5f9/Bulk_RNA-Seq_Tutorials/Bulk_RNA-Seq_Mouse/Tutorial_2_DEG_mouse.ipynb) for DEG analysis were created using this extended full dataset tutorial workflow.\n",
    "\n",
    "![Mouse Bulk RNA-seq workflow](../../images/Mouse_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Explore an example Bulk RNA-sequencing dataset\n",
    "- Understand the workflow of generating read counts, including:\n",
    "    - Accessing SRA metadata\n",
    "    - Quality control\n",
    "    - Adapter trimming\n",
    "    - Read mapping\n",
    "    - Counting mapped reads\n",
    "    - Quanitify gene expression levels\n",
    "- Report expression of the top 10 highly expressed genes\n",
    "- Combine read count files and store in AWS S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> NOTE: This Jupyter Notebook was developed to run within a customized container on AWS with all software and tools pre-configured. If running without this customized container, you will need to install tools using the Miniforge environment setup instructions below before moving on to Step 2.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Container: Install Miniforge and Workflow Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miniforge is a lightweight Conda distribution that offers a streamlined installation process and efficient package management. It provides access to a vast repository of packages.\n",
    "\n",
    "The following code performs these steps:\n",
    "- Downloads Miniforge or Mambaforge (you can use either based on preference)\n",
    "- Installs Miniforge (or Mambaforge) - no need to install conda since mamba will be available immediately\n",
    "- Using miniforge and bioconda, installs the tools that will be used in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Tip: If using the Miniforge install, run the following code cells by removing the %%script false command. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Download Miniforge or Mambaforge (you can use either based on preference)\n",
    "!curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\n",
    "\n",
    "# Install Miniforge (or Mambaforge) - no need to install conda since mamba will be available immediately\n",
    "!bash Miniforge3-$(uname)-$(uname -m).sh -b -u -p $HOME/miniforge > /dev/null\n",
    "!date +\"%T\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using mambaforge and bioconda, install the tools that will be used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Update PATH to point to the Miniforge (or Mambaforge) bin files\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"HOME\"]+\"/miniforge/bin\"\n",
    "\n",
    "#now we can easily use 'mamba' command to install software \n",
    "!mamba install -y -c conda-forge -c bioconda trimmomatic fastqc multiqc sql-magic entrez-direct gffread parallel-fastq-dump sra-tools sql-magic pyathena samtools star rsem entrez-direct subread pigz -y > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "## If running from a container, as noted above, start with <b> STEP 2 </b> below:\n",
    "## STEP 2: Define Threads & Setup Directory Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the number of available threads based on the VM. This is useful for later tools such as trimmomatic, or STAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "THREADS = max(1, num_cores - 1)\n",
    "\n",
    "print(\"Number of threads:\", THREADS)\n",
    "os.environ[\"THREADS\"] = str(THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of directories in the sra-data-athena to store the reads, reference sequence files, and output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $HOMEDIR\n",
    "!echo $PWD\n",
    "!mkdir -p data\n",
    "!mkdir -p data/trunc_rawfastq\n",
    "!mkdir -p data/trimmed\n",
    "!mkdir -p data/fastqc_samples/\n",
    "!mkdir -p data/reference\n",
    "!mkdir -p data/aligned_bam\n",
    "!mkdir -p data/rsem_reference/mouse_rsem_reference\n",
    "!mkdir -p data/rsem_output\n",
    "!mkdir -p data/reference/STAR_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Downloading relevant FASTQ files using the SRA Toolkit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need to download the relevant fastq files.\n",
    "\n",
    "Because these files can be large, the process of downloading and extracting fastq files can be quite lengthy.\n",
    "\n",
    "We will be downloading all samples from this project using the SRA Toolkit from the NCBI's SRA (Sequence Read Archive). However, first we need to find the associated accession numbers in order to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1: Finding run accession numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SRA stores sequence data in terms of runs, (SRR stands for Sequence Read Run). To download runs, we will need the accession ID for each run we wish to download.\n",
    "\n",
    "The Mittenbühler MJ et al., project contains 8 runs. To make it easier, these are the run IDs associated with this project:\n",
    "\n",
    "- SRR21972730\n",
    "- SRR21972729\n",
    "- SRR21972728\n",
    "- SRR21972727\n",
    "- SRR21972725\n",
    "- SRR21972724\n",
    "- SRR21972723\n",
    "- SRR21972726\n",
    "\n",
    "In this case, all these runs belong to the Bioproject PRJNA892075.\n",
    "\n",
    "Sequence run experiments can be searched using the SRA database on the NCBI website; and article-specific sample run information can be found in the supplementary section of that article.\n",
    "\n",
    "For instance, here, the the authors posted a link to the sequence data GSE (Gene Series number), GSE164210. This leads to the appropriate 'Gene Expression Omnibus' page where, among other useful files and information, the relevant SRA database link can be found.\n",
    "\n",
    "Once the accession numbers are located, one can make a text file containing the list of accession IDs however they like.\n",
    "\n",
    "Once again, to make things easier, we have made a .txt with these IDs that you can simply download here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!esearch -db sra -query \"PRJNA892075\" | efetch -format runinfo | cut -d',' -f1 | tail -n +2 > accs.txt\n",
    "!cat accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.2 Finding run accession numbers using Athena (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athena is a serverless query engine that allows you to analyze data stored in Amazon S3 using standard SQL. It offers several advantages over traditional SRA Toolkit, making it a more efficient and scalable solution for large-scale RNA-seq data analysis.\n",
    "\n",
    "Using Athena to access metadata is optional, but allows you to query large SRA metadata directly from AWS without needing to download and process files locally, making it faster and more scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "# Use the correct argument name: s3_staging_dir\n",
    "conn = connect(s3_staging_dir='s3://nigms-sandbox/bulk-scRNAseq/', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Glue client\n",
    "glue_client = boto3.client('glue', region_name='us-east-1')\n",
    "\n",
    "# Run the crawler\n",
    "crawler_name = 'sra_crawler'  # Use your crawler's name\n",
    "glue_client.start_crawler(Name=crawler_name)\n",
    "\n",
    "print(f\"Crawler {crawler_name} started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM AwsDataCatalog.srametadata.metadata\n",
    "WHERE bioproject = 'PRJNA892075'\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the SRR column to a text file\n",
    "with open('accs.txt', 'w') as f:\n",
    "    accs = df['acc'].to_string(header=False, index=False)\n",
    "    f.write(accs)\n",
    "    \n",
    "#print the text file\n",
    "!cat accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.3 Using the SRA-toolkit for a single sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet demonstrates how to download and preprocess single SRA data using the prefetch and fasterq-dump commands. First, prefetch downloads the specified SRA file and then, fasterq-dump converts the SRA file into paired-end FASTQ files. Finally, the generated FASTQ files are compressed using pigz to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for SRA download:\n",
    "!prefetch SRR21972723 -O data/raw_fastq -f yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sra to fastq\n",
    "!fasterq-dump data/raw_fastq/SRR21972723 -f -O data/raw_fastq/ -e $THREADS\n",
    "#compress fastq to fastq.gz to save space\n",
    "!pigz -p $THREADS data/raw_fastq/SRR21972723_1.fastq\n",
    "!pigz -p $THREADS data/raw_fastq/SRR21972723_2.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.4 Downloading multiple files using the SRA-toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses prefetch to download multiple SRA files in parallel. It reads the list of SRR IDs from accs.txt, uses xargs to execute prefetch for each ID, and specifies the output directory and the -f option to create FASTQ files in the same directory as the SRA files. To speed up the download the code uses -P $THREADS option allowing parallel execution using the specified number of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat accs.txt | xargs -P $THREADS -I {} prefetch {} -O data/raw_fastq -f yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.5 Converting Multiple SRA files to Fastq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the SRA files will be processed in parallel using parallel-fastq-dump. Each SRR ID from accs.txt will be read, and xargs will be used to execute parallel-fastq-dump for each SRA ID. This will result in the creation of two paired-end FASTQ files for each SRR ID, which will be compressed into a .gz file to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!for x in `cat accs.txt`; do fasterq-dump -f -O data/raw_fastq -e $THREADS -m 4G data/raw_fastq/$x/$x.sra; done\n",
    "\n",
    "##example of how to alternatively do the above process with parallel-fastq-dump using piping\n",
    "!cat accs.txt | xargs -I {} parallel-fastq-dump -O data/raw_fastq/ --tmpdir . --threads $THREADS --gzip --split-files --sra-id {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, it is good practice to turn .fastq files into .fastq.gz files to save space.\n",
    "\n",
    "In our case, we will actually need to concatenate the fastq files later on, and so will zip after this.\n",
    "\n",
    "The non-redundant SRA files can also be deleted to save more space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find and delete all SRR subfolders in the raw_fastq directory\n",
    "!find data/raw_fastq -type d -name 'SRR*' -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.6 Download reference transcriptome files that will be used by STAR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step downloads and prepares the reference data needed for your RNA-Seq analysis. It retrieves three essential files:\n",
    "\n",
    "- **Mouse genome (Mus_musculus.GRCm39.dna.primary_assembly.fa.gz)**: This compressed FASTA file contains the complete mouse genome sequence, that will be used as the reference for aligning your RNA-seq reads.\n",
    "- **Mouse gene annotations (Mus_musculus.GRCm39.104.gtf.gz)**: This compressed GTF file provides information about the genes and transcripts in the mouse genome, including their locations and structures. This data will crucial for interpreting the aligned RNA-Seq reads and understanding what genes are expressed in each.\n",
    "- **Mouse feature table (GCF_000001635.27_GRCm39_feature_table.txt.gz)**: This compressed table provides additional annotations for the mouse genome features, potentially including information about gene functions and pathways. This step will further used to analyze the differential gene expression (DEG) analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget ftp://ftp.ensembl.org/pub/release-104/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna.primary_assembly.fa.gz -O data/reference/mouse_genome.fa.gz\n",
    "!wget ftp://ftp.ensembl.org/pub/release-104/gtf/mus_musculus/Mus_musculus.GRCm39.104.gtf.gz -O data/reference/mouse_annotation.gtf.gz\n",
    "!wget -O data/reference/mouse_feature_table.txt.gz https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/635/GCF_000001635.27_GRCm39/GCF_000001635.27_GRCm39_feature_table.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -f data/reference/mouse_genome.fa.gz \n",
    "!gunzip -f data/reference/mouse_annotation.gtf.gz\n",
    "!gunzip -f data/reference/mouse_feature_table.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.7: Copy data file for Trimmomatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the functions of Trimmomatic is to trim adapter sequences unique to each sequencing platform. These adapter sequences are typically located within the trimmomatic installation directory in a folder called adapters.\n",
    "\n",
    "Directories of packages within conda installations can be confusing, so in the case of using conda with Trimmomatic, it may be easier to simply download or create a file with the relevant adapter sequences and store it in an easy to find directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P data/trimmed/ https://sra-data-athena.s3.amazonaws.com/reference/TruSeq3-PE.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: Run FastQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastQC is an invaluable tool that allows you to evaluate whether there are problems with a set of reads. For example, it will provide a report of whether there is any bias in the sequence composition of the reads.\n",
    "\n",
    "The below code may take a while to run. To make it run faster we can use threads to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fastqc for forward reads in parallel\n",
    "!cat accs.txt | xargs -P $THREADS -I {} fastqc \"data/raw_fastq/{}_1.fastq.gz\" -o data/fastqc/\n",
    "\n",
    "# Run fastqc for reverse reads in parallel\n",
    "!cat accs.txt | xargs -P $THREADS -I {} fastqc \"data/raw_fastq/{}_2.fastq.gz\" -o data/fastqc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastQC will output the results in HTML format, as below, for all forward and reverse reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='./data/fastqc/SRR21972724.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is best practice to look over the quality reports individually, tools like MultiQC allow one to quickly look at a combined summary of the quality reports of all fastq files.\n",
    "\n",
    "For instance, the below table shows all warnings, passes, or failures, from each FastQC report. There are other summaries created as well by MultiQC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!multiqc -f data/fastqc/\n",
    "\n",
    "import pandas as pd\n",
    "dframe = pd.read_csv(\"./multiqc_data/multiqc_fastqc.txt\", sep='\\t')\n",
    "display(dframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Merging our fastq files (Optional if there are multiple SRR per GSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the project used presents the multiple SRAs per GSM, we can use Athena to access the SRA metadata and merge the FASTQ files, the code simplifies the subsequent analysis steps and reduces the number of files to process. This can improve efficiency and reduce computational overhead.\n",
    "In this study this step was not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "# Use the correct argument name: s3_staging_dir\n",
    "conn = connect(s3_staging_dir='s3://nigms-sandbox/bulk-scRNAseq/', region_name='us-east-1')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM AwsDataCatalog.srametadata.metadata\n",
    "WHERE bioproject = 'PRJNAXXXXXXX' #Change to the Bioproject number\n",
    "AND organism = 'Mus musculus'\n",
    "\"\"\"\n",
    "df = pd.read_sql(\n",
    "    query, conn\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os so we can easily pass strings to shell commands using 'subprocess'\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "#now get the accession id's and sample id's from the created dataframe\n",
    "runs = df['acc'].values\n",
    "samples = list(set(df['acc'].values))\n",
    "\n",
    "#sort them to be in numerical order\n",
    "runs.sort()\n",
    "samples.sort()\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now iterate through the samples, \n",
    "#because there are two SRRs for each run, \n",
    "#this means corresponding SRRs indices to an index of a GSM will be\n",
    "#gsm index *2, and *2+1 \n",
    "for index, item in enumerate(samples):\n",
    "    \n",
    "    #concatenate the two SRRs\n",
    "    os.system(f\"cat data/raw_fastq/{runs[index*2]}_1.fastq data/raw_fastq/{runs[index*2+1]}_1.fastq > data/raw_fastq/{samples[index]}_1.fastq\")\n",
    "    #delete the previous fastq files to save space\n",
    "    os.system(f\"rm data/raw_fastq/{runs[index*2]}_1.fastq\")\n",
    "    os.system(f\"rm data/raw_fastq/{runs[index*2+1]}_1.fastq\")\n",
    "    #zip the merged fastq file to save more space\n",
    "    os.system(f\"gzip data/raw_fastq/{samples[index]}_1.fastq\")\n",
    "    \n",
    "    #repeat for reverse reads\n",
    "    os.system(f\"cat data/raw_fastq/{runs[index*2]}_2.fastq data/raw_fastq/{runs[index*2+1]}_2.fastq > data/raw_fastq/{samples[index]}_2.fastq\")\n",
    "    \n",
    "    os.system(f\"rm data/raw_fastq/{runs[index*2]}_2.fastq\")\n",
    "    os.system(f\"rm data/raw_fastq/{runs[index*2+1]}_2.fastq\")  \n",
    "   \n",
    "    #its good practice to zip files to save space\n",
    "    os.system(f\"gzip data/raw_fastq/{samples[index]}_2.fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since our files will now be samples, not SRRs we can write a new text file to use for downstream batch processes.\n",
    "#we can use the DF we made in the previous cell.\n",
    "with open('samples.txt', 'w') as f:\n",
    "    df = df.sort_values(by='sample_name', ascending=True)\n",
    "    samples = df['acc'].unique()\n",
    "    samples = '\\n'.join(map(str, samples))\n",
    "    f.write(samples)\n",
    "    \n",
    "!cat samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: Run Trimmomatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trimmomatic will trim off any adapter sequences or low quality sequence it detects in the FASTQ files.\n",
    "\n",
    "Using piping and our original list, it is possible to queue up a batch run of Trimmomatic for all our files, note that this is a different way to run a loop compared with what we did before.\n",
    "\n",
    "The below code may take approximately 30 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat accs.txt | xargs -I {} \\\n",
    "trimmomatic PE -threads $THREADS \\\n",
    "'data/raw_fastq/{}_1.fastq.gz' 'data/raw_fastq/{}_2.fastq.gz' \\\n",
    "'data/trimmed/{}_1_trimmed.fastq' 'data/trimmed/{}_1_trimmed_unpaired.fastq' \\\n",
    "'data/trimmed/{}_2_trimmed.fastq' 'data/trimmed/{}_2_trimmed_unpaired.fastq' \\\n",
    "ILLUMINACLIP:data/trimmed/TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7: Run FastQC\n",
    "It's best practice to run FastQC after trimming. However, you may decide to run FastQC only once, before or after trimming.\n",
    "\n",
    "We will proceed with only the forward reads -- this is because, looking at Trimmomatic, there were very few 'orphaned' reads. That is to say, most forward and reverse reads were successfully paired together. Because we are just trying to map to a reference genome, the read lengths of the forward reads alone, in this case, around 60 million basepairs, should be sufficient.\n",
    "\n",
    "The below code may take around 15-20 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FastQC\n",
    "!cat accs.txt | xargs -P $THREADS -I {} fastqc data/trimmed/{}_1_trimmed.fastq data/trimmed/{}_2_trimmed.fastq -o data/fastqc_samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: Run MultiQC\n",
    "MultiQC reads in the FastQC reports and generates a compiled report for all the analyzed FASTQ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!multiqc -f data/fastqc_samples/\n",
    "!multiqc -f -o data/multiqc_samples/ data/fastqc_samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: Preparing the STAR-Compatible RSEM Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command prepares a reference genome and annotation files for RNA-Seq analysis using RSEM (RNA-Seq by Expectation-Maximization) and STAR (Spliced Transcripts Alignment to a Reference). It generates files needed to quantify gene and isoform expression. The rsem-prepare-reference function takes a GTF file with gene annotations (mouse_annotation.gtf) and a FASTA file with the reference genome sequence (mouse_genome.fa). It processes these files to create a reference, saving the output in the mouse_reference directory. The --star option ensures the reference is compatible with STAR for efficient transcriptome alignment. The -p $THREADS option sets the number of threads used for parallel processing, speeding up the preparation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the reference genome\n",
    "!rsem-prepare-reference --gtf data/reference/mouse_annotation.gtf --star -p $THREADS data/reference/mouse_genome.fa mouse_reference > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 10: Run STAR for Alignment, Prepare and Run RSEM for Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script automates RNA-Seq gene expression quantification using RSEM and STAR. It reads SRR accession IDs from accs.txt, saves results in data/rsem_output, and runs rsem-calculate-expression for each ID. It uses paired-end trimmed FASTQ files from data/trimmed/ and a STAR-aligned RSEM reference (mouse_reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure you set the path to the RSEM binary\n",
    "# Read the SRR accessions from the file\n",
    "with open('accs.txt', 'r') as f:\n",
    "    srr_accessions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"data/rsem_output\"\n",
    "\n",
    "# Loop through each SRR accession and run rsem-calculate-expression\n",
    "for srr in srr_accessions:\n",
    "    !rsem-calculate-expression -p $THREADS --paired-end --star \\\n",
    "    data/trimmed/{srr}_1_trimmed.fastq data/trimmed/{srr}_2_trimmed.fastq mouse_reference data/rsem_output/{srr} > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 11: Report the top 10 most highly expressed genes in the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in each wild-type sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to RSEM results directory\n",
    "rsem_results_dir = 'data/rsem_output'\n",
    "\n",
    "# Loop through each file in accs.txt\n",
    "for srr_id in open('accs.txt'):\n",
    "    srr_id = srr_id.strip()  # Remove newline character\n",
    "    rsem_result_file = f'{rsem_results_dir}/{srr_id}.genes.results'\n",
    "\n",
    "    # Load the RSEM results into a Pandas DataFrame\n",
    "    df = pd.read_csv(rsem_result_file, sep='\\t')\n",
    "\n",
    "    # Sort the DataFrame by TPM values in descending order and get the top 10 genes\n",
    "    top_10_genes = df.sort_values(by='TPM', ascending=False).head(10)\n",
    "\n",
    "    # Print the top 10 genes with their TPM values\n",
    "    print(f\"Top 10 Genes by TPM for {srr_id}:\")\n",
    "    print(top_10_genes[['gene_id', 'TPM']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 12: Report the expression of ENSMUSG00000064351 for each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the wild-type sample. The fields in the RSEM `genes.results` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to RSEM results directory\n",
    "rsem_results_dir = 'data/rsem_output'\n",
    "\n",
    "# Target gene ID\n",
    "target_gene = 'ENSMUSG00000064351'\n",
    "\n",
    "# Loop through each file in accs.txt\n",
    "for srr_id in open('accs.txt'):\n",
    "    srr_id = srr_id.strip()  # Remove newline character\n",
    "    rsem_result_file = f'{rsem_results_dir}/{srr_id}.genes.results'\n",
    "\n",
    "    # Load the RSEM results into a Pandas DataFrame\n",
    "    df = pd.read_csv(rsem_result_file, sep='\\t')\n",
    "\n",
    "    # Filter for the target gene\n",
    "    target_gene_data = df[df['gene_id'] == target_gene]\n",
    "\n",
    "    # Print the target gene's TPM value for the SRR ID\n",
    "    print(f\"TPM for {target_gene} in {srr_id}: {target_gene_data['TPM'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 13: Export Read counts to AWS S3 Bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code effectively extracts gene expression data from RSEM output files and stores them in a structured format on an S3 bucket. This data will be accessible for further analysis in Tutorial 2 and Tutorial 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# Define the path to your RSEM output directory\n",
    "rsem_output_path = \"data/rsem_output\"\n",
    "\n",
    "# Define the S3 bucket and output path\n",
    "s3_bucket = \"nigms-sandbox/bulk-scRNAseq\"\n",
    "s3_output_path = \"readcounts/\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Get a list of all .genes.results files in the directory\n",
    "genes_files = [f for f in os.listdir(rsem_output_path) if f.endswith('.genes.results')]\n",
    "\n",
    "# Loop through each file to extract gene ID, expected counts, and gene length\n",
    "for file in genes_files:\n",
    "    file_path = os.path.join(rsem_output_path, file)\n",
    "    \n",
    "    # Read the .genes.results file\n",
    "    rsem_data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "    # Check if the necessary columns exist\n",
    "    if all(col in rsem_data.columns for col in [\"gene_id\", \"expected_count\", \"length\"]):\n",
    "        # Create a new dataframe with required columns\n",
    "        result_data = rsem_data[[\"gene_id\", \"expected_count\", \"length\"]]\n",
    "        result_data.columns = [\"GeneID\", \"Count\", \"GeneLength\"]\n",
    "\n",
    "        # Define the output filename based on the input file name\n",
    "        output_file_name = f\"{os.path.splitext(file)[0]}.txt\"\n",
    "        s3_output_file_path = f\"{s3_output_path}{output_file_name}\"\n",
    "\n",
    "        # Convert the DataFrame to a CSV string\n",
    "        csv_buffer = result_data.to_csv(sep=\"\\t\", index=False)\n",
    "\n",
    "        # Upload the result directly to S3\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=s3_output_file_path, Body=csv_buffer)\n",
    "\n",
    "    else:\n",
    "        print(f\"Warning: Required columns are missing in file: {file}\")\n",
    "\n",
    "# Optionally, print a message indicating completion\n",
    "print(\"Extraction and file creation complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 14: Save Merged Read Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code combines multiple RSEM gene count files into a single, unified file, making it easier to analyze and visualize the gene expression data. This files was also uploaded to S3 Bucket to allow further analysis in other Tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the RSEM quantification results directory exists\n",
    "!mkdir -p data/rsem_output\n",
    "\n",
    "# Merge RSEM results by gene counts (similar to Salmon's numreads merge)\n",
    "!rsem-generate-data-matrix data/rsem_output/*.genes.results > data/rsem_output/merged_gene_counts.txt\n",
    "\n",
    "# Optionally, rename the columns based on the samples\n",
    "# If you want to assign your GSM identifiers or any other custom names, edit the header.\n",
    "!sed -i \"1s/.*/Name\\tGSM6658439\\tGSM6658438\\tGSM6658435\\tGSM6658441\\tGSM6658433\\tGSM6658431\\tGSM6658429\\tGSM6658427/\" data/rsem_output/merged_gene_counts.txt\n",
    "\n",
    "# Remove any unnecessary prefixes like 'gene-' or 'rna-' for easier formatting\n",
    "!sed -i \"s/gene-//g\" data/rsem_output/merged_gene_counts.txt\n",
    "!sed -i \"s/rna-//g\" data/rsem_output/merged_gene_counts.txt\n",
    "\n",
    "# Show a preview of the merged quantification file\n",
    "!head data/rsem_output/merged_gene_counts.txt\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Define the file path and S3 bucket details\n",
    "file_path = \"data/rsem_output/merged_gene_counts.txt\"\n",
    "bucket_name = \"nigms-sandbox/bulk-scRNAseq\"\n",
    "s3_key = \"readcounts/merged_gene_counts.txt\"\n",
    "\n",
    "# Initialize an S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Upload the file to the specified S3 bucket\n",
    "try:\n",
    "    s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "    print(f\"File {file_path} uploaded successfully to {bucket_name}/{s3_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")\n",
    "\n",
    "# Define the file paths and S3 bucket details\n",
    "rsem_output_path = \"data/rsem_output\"\n",
    "feature_table_path = \"data/reference/mouse_feature_table.txt\"\n",
    "bucket_name = \"nigms-sandbox/bulk-scRNAseq\"\n",
    "s3_output_path = \"readcounts/\"\n",
    "s3_feature_table_path = \"reference/mouse_feature_table.txt\"\n",
    "\n",
    "# Upload the gene count file\n",
    "s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "\n",
    "# Upload the feature table file\n",
    "s3_client.upload_file(feature_table_path, bucket_name, s3_feature_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 15: Save RSEM reference and STAR index to AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp data/rsem_reference s3://nigms-sandbox/bulk-scRNAseq/reference/rsem_reference/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we covered the following key concepts and workflow steps:\n",
    "\n",
    "- **Bulk RNA-seq Preprocessing**: Downloading the full dataset and metadata using the SRA Toolkit and Athena, and setting up directories\n",
    "- **Quality Control**: Use FastQC and MultiQC to assess the quality of reads in the dataset and combine results for all samples to generate a comprehensive overview of quality metrics across multiple samples.\n",
    "- **Adapter Trimming**: Learn how to use Trimmomatic to remove adapter sequences and low-quality bases from FASTQ reads.\n",
    "- **Read Mapping and Quantification**: Understand the purpose of indexing and learn how to use STAR to create an index of the reference genome for efficient read mapping. Map reads to reference genome and quantify gene expression levels using RSEM.\n",
    "- **Storing Data Outputs**: Learn how to merge read count data and store it in an AWS S3 bucket to be utilized in subsequent analysis in Tutorial 2 and 3.\n",
    "\n",
    "In summary, this Jupyter Notebook provided a hands-on demonstration of an extended Bulk RNA-Seq analysis workflow, guiding users through essential steps such as obtaining sequencing data from the Sequence Read Archive using the SRA Toolkit and Athena, read trimming with Trimmomatic, quality control with FastQC and MultiQC, reference genome indexing with STAR, read mapping and quantification using RSEM, and storing data on an AWS S3 bucket. By processing the full set of reads from a mouse dataset, we were able to observe the expression levels of the top 10 most highly expressed genes in each sample. This workflow serves as a foundation for more advanced analyses, and further resources are available for utilizing R/DESeq2 for differential gene expression analysis using the read counts generated in this tutorial, and using NetAct for transcription factor network analysis. Ultimately, this tutorial equips users with the basic skills to analyze RNA-seq data and to understand the core components of a typical RNA-seq pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"workflow\">Additional Workflows</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have read counts per gene, feel free to explore the R workflow which creates plots and analyses using these readcount files, or try other alternate workflows for creating read count files, such as using snakemake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Workflow One:](Tutorial_1_subsampling_mouse-miniforge.ipynb) A short introduction to downloading and mapping sequences to a mouse genome using STAR and RSEM.\n",
    "\n",
    "\n",
    "[Workflow Two (DEG Analysis):](Tutorial_2_DEG_Analysis_mouse.ipynb) Using Deseq2 and R to conduct clustering and differential gene expression analysis.\n",
    "\n",
    "[Workflow Three (Network Analysis):](Tutorial_3_NetAct.ipynb) Using NetAct and R to conduct transcription factor network analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
